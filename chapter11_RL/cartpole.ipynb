{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用增强学习玩车杆游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "玩一局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新一局游戏 初始观测 = [-0.03172386  0.01544278  0.03376981 -0.0426311 ], 其他信息 = {}\n",
      "0: 动作 = 1\n",
      "0: 观测 = [-0.031415    0.21006462  0.03291719 -0.32447097], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "1: 动作 = 1\n",
      "1: 观测 = [-0.02721371  0.40470278  0.02642777 -0.60659415], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "2: 动作 = 1\n",
      "2: 观测 = [-0.01911965  0.59944546  0.01429588 -0.89083725], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "3: 动作 = 1\n",
      "3: 观测 = [-0.00713074  0.79437053 -0.00352086 -1.1789923 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "4: 动作 = 0\n",
      "4: 观测 = [ 0.00875667  0.5992945  -0.02710071 -0.88741505], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "5: 动作 = 0\n",
      "5: 观测 = [ 0.02074256  0.40455067 -0.04484901 -0.6033732 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "6: 动作 = 0\n",
      "6: 观测 = [ 0.02883357  0.21008372 -0.05691648 -0.32514733], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "7: 动作 = 1\n",
      "7: 观测 = [ 0.03303524  0.40596792 -0.06341942 -0.63522226], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "8: 动作 = 0\n",
      "8: 观测 = [ 0.0411546   0.21178518 -0.07612386 -0.3631665 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "9: 动作 = 0\n",
      "9: 观测 = [ 0.04539031  0.01782305 -0.0833872  -0.0954247 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "10: 动作 = 0\n",
      "10: 观测 = [ 0.04574677 -0.17601089 -0.08529569  0.1698285 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "11: 动作 = 1\n",
      "11: 观测 = [ 0.04222655  0.02022179 -0.08189912 -0.14849696], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "12: 动作 = 1\n",
      "12: 观测 = [ 0.04263099  0.21641521 -0.08486906 -0.4658521 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "13: 动作 = 1\n",
      "13: 观测 = [ 0.04695929  0.41262737 -0.0941861  -0.7840326 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "14: 动作 = 1\n",
      "14: 观测 = [ 0.05521184  0.6089087  -0.10986675 -1.1047994 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "15: 动作 = 1\n",
      "15: 观测 = [ 0.06739001  0.80529034 -0.13196275 -1.4298316 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "16: 动作 = 1\n",
      "16: 观测 = [ 0.08349582  1.0017716  -0.16055937 -1.7606757 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "17: 动作 = 1\n",
      "17: 观测 = [ 0.10353125  1.1983057  -0.19577289 -2.0986872 ], 本步得分 = 1.0, 结束指示 = False, 截断指示 = False, 其他信息 = {}\n",
      "18: 动作 = 1\n",
      "18: 观测 = [ 0.12749736  1.3947843  -0.23774663 -2.4449654 ], 本步得分 = 1.0, 结束指示 = True, 截断指示 = False, 其他信息 = {}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0') # 获得游戏环境\n",
    "observation, info = env.reset() # 复位游戏环境,新一局游戏开始\n",
    "print ('新一局游戏 初始观测 = {}, 其他信息 = {}'.format(observation, info))\n",
    "for t in range(200):\n",
    "    env.render()\n",
    "    action = env.action_space.sample() # 随机选择动作\n",
    "    print ('{}: 动作 = {}'.format(t, action))\n",
    "    observation, reward, terminated, truncated, info = env.step(action) # 执行行为\n",
    "    print ('{}: 观测 = {}, 本步得分 = {}, 结束指示 = {}, 截断指示 = {}, 其他信息 = {}'.format(\n",
    "            t, observation, reward, terminated, truncated, info))\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "玩多局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0局得分 = 22.0\n",
      "第1局得分 = 16.0\n",
      "第2局得分 = 16.0\n",
      "第3局得分 = 41.0\n",
      "第4局得分 = 29.0\n",
      "第5局得分 = 58.0\n",
      "第6局得分 = 25.0\n",
      "第7局得分 = 16.0\n",
      "第8局得分 = 13.0\n",
      "第9局得分 = 32.0\n",
      "第10局得分 = 23.0\n",
      "第11局得分 = 18.0\n",
      "第12局得分 = 18.0\n",
      "第13局得分 = 10.0\n",
      "第14局得分 = 46.0\n",
      "第15局得分 = 21.0\n",
      "第16局得分 = 16.0\n",
      "第17局得分 = 14.0\n",
      "第18局得分 = 16.0\n",
      "第19局得分 = 16.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "n_episode = 20\n",
    "for i_episode in range(n_episode):\n",
    "    observation, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        # env.render()\n",
    "        action = env.action_space.sample() # 随机选\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = observation\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    print ('第{}局得分 = {}'.format(i_episode, episode_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cart Pole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "搭建 DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0], 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, env.action_space.n)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def act(model, state, epsilon):\n",
    "    if random.random() > epsilon: # 选最大的\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_value = model.forward(state)\n",
    "        action = q_value.max(1)[1].item()\n",
    "    else: # 随便选\n",
    "        action = random.randrange(env.action_space.n)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon值不断下降\n",
    "import math\n",
    "def calc_epsilon(t, epsilon_start=1.0,\n",
    "        epsilon_final=0.01, epsilon_decay=500):\n",
    "    epsilon = epsilon_final + (epsilon_start - epsilon_final) \\\n",
    "            * math.exp(-1. * t / epsilon_decay)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最近历史缓存\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, terminated = zip( \\\n",
    "                *random.sample(self.buffer, batch_size))\n",
    "        concat_state = np.concatenate(state)\n",
    "        concat_next_state = np.concatenate(next_state)\n",
    "        return concat_state, action, reward, concat_next_state, terminated\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0局收益 = 30.0\n",
      "第1局收益 = 11.0\n",
      "第2局收益 = 62.0\n",
      "第3局收益 = 14.0\n",
      "第4局收益 = 23.0\n",
      "第5局收益 = 16.0\n",
      "第6局收益 = 17.0\n",
      "第7局收益 = 24.0\n",
      "第8局收益 = 18.0\n",
      "第9局收益 = 16.0\n",
      "第10局收益 = 22.0\n",
      "第11局收益 = 21.0\n",
      "第12局收益 = 21.0\n",
      "第13局收益 = 43.0\n",
      "第14局收益 = 54.0\n",
      "第15局收益 = 26.0\n",
      "第16局收益 = 48.0\n",
      "第17局收益 = 47.0\n",
      "第18局收益 = 32.0\n",
      "第19局收益 = 36.0\n",
      "第20局收益 = 40.0\n",
      "第21局收益 = 27.0\n",
      "第22局收益 = 76.0\n",
      "第23局收益 = 25.0\n",
      "第24局收益 = 21.0\n",
      "第25局收益 = 42.0\n",
      "第26局收益 = 44.0\n",
      "第27局收益 = 33.0\n",
      "第28局收益 = 32.0\n",
      "第29局收益 = 48.0\n",
      "第30局收益 = 24.0\n",
      "第31局收益 = 47.0\n",
      "第32局收益 = 125.0\n",
      "第33局收益 = 44.0\n",
      "第34局收益 = 49.0\n",
      "第35局收益 = 79.0\n",
      "第36局收益 = 120.0\n",
      "第37局收益 = 117.0\n",
      "第38局收益 = 120.0\n",
      "第39局收益 = 56.0\n",
      "第40局收益 = 102.0\n",
      "第41局收益 = 200.0\n",
      "第42局收益 = 80.0\n",
      "第43局收益 = 137.0\n",
      "第44局收益 = 127.0\n",
      "第45局收益 = 113.0\n",
      "第46局收益 = 84.0\n",
      "第47局收益 = 106.0\n",
      "第48局收益 = 200.0\n",
      "第49局收益 = 200.0\n",
      "第50局收益 = 147.0\n",
      "第51局收益 = 143.0\n",
      "第52局收益 = 200.0\n",
      "第53局收益 = 200.0\n",
      "第54局收益 = 133.0\n",
      "第55局收益 = 189.0\n",
      "第56局收益 = 116.0\n",
      "第57局收益 = 149.0\n",
      "第58局收益 = 200.0\n",
      "第59局收益 = 200.0\n",
      "第60局收益 = 200.0\n",
      "第61局收益 = 187.0\n",
      "第62局收益 = 200.0\n",
      "第63局收益 = 200.0\n",
      "第64局收益 = 200.0\n",
      "第65局收益 = 200.0\n",
      "第66局收益 = 200.0\n",
      "第67局收益 = 200.0\n",
      "第68局收益 = 200.0\n",
      "第69局收益 = 200.0\n",
      "第70局收益 = 152.0\n",
      "第71局收益 = 18.0\n",
      "第72局收益 = 11.0\n",
      "第73局收益 = 188.0\n",
      "第74局收益 = 200.0\n",
      "第75局收益 = 200.0\n",
      "第76局收益 = 200.0\n",
      "第77局收益 = 200.0\n",
      "第78局收益 = 200.0\n",
      "第79局收益 = 200.0\n",
      "第80局收益 = 200.0\n",
      "第81局收益 = 200.0\n",
      "第82局收益 = 200.0\n",
      "第83局收益 = 200.0\n",
      "第84局收益 = 200.0\n",
      "第85局收益 = 200.0\n",
      "第86局收益 = 200.0\n",
      "第87局收益 = 200.0\n",
      "第88局收益 = 200.0\n",
      "第89局收益 = 200.0\n",
      "第90局收益 = 200.0\n",
      "第91局收益 = 200.0\n",
      "第92局收益 = 200.0\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "episode_rewards = [] # 各局得分,用来判断训练是否完成\n",
    "t = 0 # 训练步数,用于计算epsilon\n",
    "\n",
    "while True:\n",
    "\n",
    "    # 开始新的一局\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    while True:\n",
    "        epsilon = calc_epsilon(t)\n",
    "        action = act(model, state, epsilon)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "\n",
    "            # 计算时间差分误差\n",
    "            sample_state, sample_action, sample_reward, sample_next_state, \\\n",
    "                    sample_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "            sample_state = torch.tensor(sample_state, dtype=torch.float32)\n",
    "            sample_action = torch.tensor(sample_action, dtype=torch.int64)\n",
    "            sample_reward = torch.tensor(sample_reward, dtype=torch.float32)\n",
    "            sample_next_state = torch.tensor(sample_next_state,\n",
    "                    dtype=torch.float32)\n",
    "            sample_done = torch.tensor(sample_done, dtype=torch.float32)\n",
    "\n",
    "            next_qs = model(sample_next_state)\n",
    "            next_q, _ = next_qs.max(1)\n",
    "            expected_q = sample_reward + gamma * next_q * (1 - sample_done)\n",
    "\n",
    "            qs = model(sample_state)\n",
    "            q = qs.gather(1, sample_action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            td_error = expected_q - q\n",
    "\n",
    "            # 计算 MSE 损失\n",
    "            loss = td_error.pow(2).mean()\n",
    "\n",
    "            # 根据损失改进网络\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        if terminated or truncated: # 本局结束\n",
    "            i_episode = len(episode_rewards)\n",
    "            print ('第{}局收益 = {}'.format(i_episode, episode_reward))\n",
    "            episode_rewards.append(episode_reward)\n",
    "            break\n",
    "\n",
    "    if len(episode_rewards) > 20 and np.mean(episode_rewards[-20:]) > 195:\n",
    "        break # 训练结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用\n",
    "（固定 $\\epsilon$ 的值为0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0局得分 = 200.0\n",
      "第1局得分 = 200.0\n",
      "第2局得分 = 200.0\n",
      "第3局得分 = 200.0\n",
      "第4局得分 = 200.0\n",
      "第5局得分 = 200.0\n",
      "第6局得分 = 200.0\n",
      "第7局得分 = 200.0\n",
      "第8局得分 = 200.0\n",
      "第9局得分 = 200.0\n",
      "第10局得分 = 200.0\n",
      "第11局得分 = 200.0\n",
      "第12局得分 = 200.0\n",
      "第13局得分 = 200.0\n",
      "第14局得分 = 200.0\n",
      "第15局得分 = 200.0\n",
      "第16局得分 = 200.0\n",
      "第17局得分 = 200.0\n",
      "第18局得分 = 200.0\n",
      "第19局得分 = 200.0\n"
     ]
    }
   ],
   "source": [
    "n_episode = 20\n",
    "for i_episode in range(n_episode):\n",
    "    observation, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        # env.render()\n",
    "        action = act(model, observation, 0)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = observation\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    print ('第{}局得分 = {}'.format(i_episode, episode_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
